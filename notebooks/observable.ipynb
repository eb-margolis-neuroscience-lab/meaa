{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6iQKYaw50yB"
   },
   "source": [
    "## Med64 QQ Export D3 Data ##\n",
    "\n",
    "Prior name of file: `med64_qq_fr_nwb.ipynb`\n",
    "\n",
    "To use with CoLab notebooks, upload the data files manually. \n",
    "This will downsample the data and export in a file for uploading to observable.\n",
    "\n",
    "This version works with the Spyking Circus, NWB, YAML Parameter files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NCVsWX8eDeNi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import yaml\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "from scipy import signal, stats\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "# meappy module\n",
    "from meappy.med64_data import *\n",
    "from meappy.parameter_yaml import USER, USER_PATHS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "cmap = cm.get_cmap('tab20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_expt_files(yaml_file):    \n",
    "#     with open(yaml_file, 'r') as file:\n",
    "#         med64_data = yaml.safe_load(file)\n",
    "\n",
    "#     # pprint(med64_data, sort_dicts=False, indent=4)\n",
    "    \n",
    "#     paths = med64_data['paths']\n",
    "#     slice_path = os.path.join(paths['base'], paths['protocol'], paths['slice'])\n",
    "    \n",
    "#     slice_name = paths['slice']\n",
    "#     tx_file = os.path.join(slice_path, paths['treatment'])\n",
    "#     unit_file = os.path.join(slice_path, paths['unit'])\n",
    "\n",
    "#     #  expt_list[0] now replaced with slice_name\n",
    "#     # tx_files now tx_file without s\n",
    "#     return  slice_name, tx_file, unit_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_better_expt_files(data_dir):\n",
    "    files = os.listdir(data_dir)\n",
    "\n",
    "    re_units = re.compile(r'^EBM_betterResults.*')\n",
    "    re_tx = re.compile(r'^treatmentinfo.*')\n",
    "    re_expr_id = re.compile(r\"([0-9]{3}_[0-9]{2}h[0-9]{2}m[0-9]{2}s)\\.['csv|mat']{3}\")\n",
    "\n",
    "    unit_files = dict()\n",
    "    tx_files = dict()\n",
    "\n",
    "    for file in files:\n",
    "        expr_id = re_expr_id.findall(file)\n",
    "        if expr_id:\n",
    "            units_file = re_units.findall(file)\n",
    "            if units_file:\n",
    "                unit_files[expr_id[0]] = units_file[0]\n",
    "            tx_file = re_tx.findall(file)\n",
    "            if tx_file:\n",
    "                tx_files[expr_id[0]] = tx_file[0]\n",
    "    expt_list = sorted(list(unit_files.keys()))\n",
    "\n",
    "    print('FOUND Data FILES:')\n",
    "    print('Experiment IDs: \\n\\t' + '\\n\\t'.join(expt_list))\n",
    "    print('Units Files: \\n\\t' + '\\n\\t'.join(unit_files.values()))\n",
    "    print('Treatment Files: \\n\\t' + '\\n\\t'.join(tx_files.values()))\n",
    "    \n",
    "    return expt_list, tx_files, unit_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_firing_rates(units_data, tx_times):\n",
    "    \"\"\"\n",
    "    get firing rate of a units in Hz\n",
    "    depricated\n",
    "    \"\"\"\n",
    "    num_units = len(units_data)\n",
    "    start_time_sec = list(tx_times.values())[0]\n",
    "    end_time_sec = list(tx_times.values())[-1]\n",
    "    duration_s = end_time_sec - start_time_sec\n",
    "    \n",
    "    mean_firing_rate_hz = dict()\n",
    "    all_unit_activity_count = 0\n",
    "    for (id, data) in units_data.items():\n",
    "        timestamps = data['timestamps']\n",
    "        activity_count = timestamps.shape[0]\n",
    "        mean_firing_rate_hz[id] = activity_count / duration_s\n",
    "        all_unit_activity_count += activity_count\n",
    "        \n",
    "    all_unit_mean_fr_hz = all_unit_activity_count / duration_s / num_units\n",
    "    return all_unit_mean_fr_hz, mean_firing_rate_hz\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "### check below for A more accurate version of get_tx_ranges  \n",
    "* Use this when I have time to modify the d3 code to display it.\n",
    "* Also, check if there are timestamps beyond the \"TTX off\" and allow for arbitrary end time to last range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS ACCURATE VERSION DONT DELETE\n",
    "\n",
    "# import operator\n",
    "\n",
    "# def get_tx_ranges(tx_times):\n",
    "#     tx_ranges = {}\n",
    "#     sorted_tx_times = sorted(tx_times.items(), key=operator.itemgetter(1))\n",
    "#     prev_tx = None\n",
    "#     prev_time = None\n",
    "#     for tx, time in sorted_tx_times:\n",
    "#         if (prev_tx != None) and (prev_time != time):\n",
    "#             tx_ranges[prev_tx.strip()] = [prev_time, time]\n",
    "#         prev_tx = tx\n",
    "#         prev_time = time\n",
    "#     return tx_ranges\n",
    "\n",
    "# get_tx_ranges(tx_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tx_ranges(tx_times):\n",
    "    all_times = sorted(list(tx_times.values()))\n",
    "    last_time_index = len(all_times) - 1\n",
    "    tx_range = dict()\n",
    "    for tx, time in tx_times.items():\n",
    "        time_index = all_times.index(time)\n",
    "        if time_index == last_time_index:\n",
    "            break\n",
    "        next_time = all_times[time_index + 1]\n",
    "        if time >= next_time:\n",
    "            continue\n",
    "        tx_range[tx.strip()] = [time, next_time]\n",
    "    return tx_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mean_firing_by_tx(units_data, tx_times):\n",
    "    \"\"\"\n",
    "    get mean firing rate of units in Hz for each treatment (tx)\n",
    "    \"\"\"\n",
    "    tx_range = get_tx_ranges(tx_times)\n",
    "    tx_mean_firing_rate_hz = dict()\n",
    "    for tx in tx_range.keys():\n",
    "        tx_mean_firing_rate_hz[tx] = {}\n",
    "    for (unit_id, data) in units_data.items():\n",
    "        for (tx, [start_time, end_time]) in tx_range.items():\n",
    "            timestamps = data['timestamps']\n",
    "            duration = end_time - start_time\n",
    "            activity_count = [ts for ts in units_data[unit_id]['timestamps'] if \n",
    "                 (ts > start_time) and (ts < end_time)]\n",
    "            tx_mean_firing_rate_hz[tx][unit_id] = len(activity_count)/duration\n",
    "    return tx_mean_firing_rate_hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_firing_rate_df(df_d3, mean_firing_by_tx):\n",
    "    \"\"\"\n",
    "    create new dataframe with firing rate data formatted to qq dataframe\n",
    "    \"\"\"\n",
    "    firing_rate_df = pd.DataFrame(columns = (df_d3.columns.tolist() + ['Firing Rate']))\n",
    "    df_index = 0\n",
    "    for tx in mean_firing_by_tx.keys():\n",
    "        for unit in mean_firing_by_tx[tx].keys():\n",
    "            fr = mean_firing_by_tx[tx][unit]\n",
    "            nan_cols = firing_rate_df.columns.shape[0] - 4\n",
    "            firing_rate_df.loc[df_index] = ([np.nan, unit, tx] + [np.nan] * nan_cols + [fr])\n",
    "            df_index += 1\n",
    "    return firing_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  next line is to create from raw file input\n",
    "def get_mean_firing_rates(unit_files, tx_files):\n",
    "    units_data, tx_times = get_expt_data(unit_files, tx_files)    \n",
    "    mean_firing_by_tx = get_mean_firing_by_tx(units_data, tx_times)\n",
    "    return mean_firing_by_tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue with QQ calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed up get_timestamp_...\n",
    "by using lookup in a table generated by array functions instead of loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp_tx(ts, tx_range):\n",
    "    for tx, times in tx_range.items():\n",
    "        if (ts >= times[0]) & (ts < times[1]):\n",
    "            return tx\n",
    "    return 'No Tx'\n",
    "        \n",
    "def get_timestamp_begin(ts, tx_range):\n",
    "    for tx, times in tx_range.items():\n",
    "        if (ts >= times[0]) & (ts < times[1]):\n",
    "            return times[0]\n",
    "    return np.nan   \n",
    "    \n",
    "def get_timestamp_end(ts, tx_range):\n",
    "    for tx, times in tx_range.items():\n",
    "        if (ts >= times[0]) & (ts < times[1]):\n",
    "            return times[1]\n",
    "    return np.nan   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(units_data, tx_times):\n",
    "    tx_range = get_tx_ranges(tx_times)\n",
    "    time_tx = partial(get_timestamp_tx, tx_range = tx_range)\n",
    "    time_begin = partial(get_timestamp_begin, tx_range = tx_range)\n",
    "    time_end = partial(get_timestamp_end, tx_range = tx_range)\n",
    "\n",
    "    df = None\n",
    "    # for loop over units\n",
    "    for unit_id in list(units_data.keys()):\n",
    "        timestamps = units_data[unit_id]['timestamps']\n",
    "\n",
    "        unit_df = pd.DataFrame(timestamps, columns=['timestamp'])\n",
    "        unit_df['unit'] = unit_id\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            df = df.append(unit_df, ignore_index=True)\n",
    "        else:\n",
    "            df = unit_df\n",
    "    \n",
    "    # end loop over units\n",
    "    df['tx'] = df['timestamp'].map(lambda x: time_tx(x))\n",
    "    df['begin'] = df['timestamp'].map(lambda x: time_begin(x))\n",
    "    df['end'] = df['timestamp'].map(lambda x: time_end(x))\n",
    "\n",
    "    ts_groups = df.groupby(by = ['unit', 'tx'])\n",
    "    df['group_idx'] = ts_groups['timestamp'].cumcount()+1\n",
    "    \n",
    "    ts_groups = df.groupby(by = ['unit', 'tx'])\n",
    "    df['cum_dist'] = ts_groups['group_idx'].apply(lambda df: df/df.count())\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_anchors(df):\n",
    "    anchors_begin = df[['unit', 'tx', 'begin', 'end']].drop_duplicates()\n",
    "    anchors_begin.dropna(inplace=True)\n",
    "    anchors_end = anchors_begin.copy()\n",
    "    \n",
    "    anchors_begin['cum_dist'] = 1\n",
    "    anchors_begin['timestamp'] = anchors_begin['end']\n",
    "\n",
    "    anchors_end['cum_dist'] = 0\n",
    "    anchors_end['timestamp'] = anchors_end['begin']\n",
    "    \n",
    "    return pd.concat([df, anchors_begin, anchors_end], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export QQ data for d3 Linked Brushing Cross-filtering    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_qq(unit_files, tx_files):\n",
    "    units_data, tx_times = get_expt_data(unit_files, tx_files)\n",
    "\n",
    "    df = build_df(units_data, tx_times)\n",
    "    df = add_anchors(df)\n",
    "\n",
    "    df = df[df['tx'] != 'No Tx'] # drop times outside treatment time ranges\n",
    "    df['x_plot'] = (df['timestamp']-df['begin'])/(df['end']-df['begin'])\n",
    "    \n",
    "    # sort zero anchors to beginning of dataframe\n",
    "    df.sort_values(by=['unit', 'x_plot'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_samples(df, max_d3_rows):\n",
    "    \"\"\"Returns the max samples per unit per treatment for d3 observable total_rows\n",
    "    \"\"\"\n",
    "    max_samples = round(max_d3_rows / df.unit.unique().size / df.tx.unique().size)\n",
    "    return max_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def downsample_data(df, max_d3_rows):\n",
    "    \"\"\"downsamples the data in each unit so that the max total size of data will\n",
    "    work in d3 plot. If more units, then data per unit is larger.\n",
    "    \"\"\"\n",
    "    max_samples = get_max_samples(df, max_d3_rows)\n",
    "    df_downsampled = None\n",
    "\n",
    "    downsample_groups = df.groupby(by=['unit', 'tx'])\n",
    "\n",
    "    for by, grp in downsample_groups:\n",
    "    #     if (by[0] == 5): # & (by[1] == 'DAMGO 500nM On'):\n",
    "        grp.sort_values(by='x_plot', inplace=True)\n",
    "        samples = grp.shape[0]\n",
    "        downsample_rate = round(np.floor(samples / max_samples))\n",
    "        downsample_rate = 1 if downsample_rate < 1 else downsample_rate\n",
    "        if isinstance(df_downsampled, pd.DataFrame):\n",
    "            df_downsampled = df_downsampled.append(grp.iloc[:-1:downsample_rate])\n",
    "        else:\n",
    "            df_downsampled = grp.iloc[:-1:downsample_rate] # don't add anchor here\n",
    "        # add final anchor now, because downsampling could have excluded it\n",
    "        df_downsampled = df_downsampled.append(grp.iloc[-1:])\n",
    "#         print(downsample_rate, samples)\n",
    "#         print(grp.iloc[::downsample_rate])\n",
    "#         print(grp.iloc[::downsample_rate].shape)\n",
    "#         print(df_downsampled.shape)\n",
    "    return df_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypoactive_unit_tx(df, activity_threshold):\n",
    "    # Depricated for activity_gaps_unit_tx\n",
    "    \"\"\"return list of tuples with (unit, tx) for low unit\n",
    "    activity that will not have enough points on d3 lines\"\"\"\n",
    "    threshold_fraction = 0.5\n",
    "    group_sizes = df.groupby(by=['unit', 'tx'])['timestamp']\n",
    "    return [i for i, v in (group_sizes.count() < (threshold_fraction * activity_threshold)).items() if v]\n",
    "\n",
    "# interp_units = hypoactive_unit_tx(df_downsampled, get_max_samples(df, max_d3_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_activity = df.groupby(by=['unit', 'tx'])[['timestamp', 'begin', 'end']]\n",
    "# num_segments = int(get_max_samples(df, max_d3_rows)/2)\n",
    "\n",
    " # activity_gaps_unit_tx(df_downsampled, get_max_samples(df, max_d3_rows)):\n",
    "    \n",
    "def groups_with_gaps(group_activity, num_segments): \n",
    "    gap_groups = []\n",
    "    for i, v in group_activity:\n",
    "#     if i[0] < 3:\n",
    "#         print(i)\n",
    "        spikes = v.timestamp.values\n",
    "        begin = v.iloc[0,:]['begin']\n",
    "        end = v.iloc[0,:]['end']\n",
    "        segments = np.linspace(begin, end, num_segments)\n",
    "        prev_seg = segments[0]\n",
    "        is_gap = False\n",
    "        for seg in segments[1:]:\n",
    "            gap_activity = [s for s in spikes if ((s > prev_seg) & (s < seg))]\n",
    "#             print(len(gap_activity))\n",
    "            if len(gap_activity) == 0:\n",
    "                is_gap = True\n",
    "        if is_gap:\n",
    "            gap_groups.append(i)\n",
    "    return gap_groups\n",
    "    \n",
    "# groups_with_gaps(group_activity, num_segments)expt_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def activity_gaps_unit_tx(df, d3_sample_threshold):\n",
    "    \"\"\"return list of tuples with (unit, tx) for unit with gaps in\n",
    "    activity that will not have enough points on d3 lines\"\"\"\n",
    "    num_segments = int(get_max_samples(df, max_d3_rows)/2)\n",
    "    group_activity = df.groupby(by=['unit', 'tx'])[['timestamp', 'begin', 'end']]\n",
    "    return groups_with_gaps(group_activity, num_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp = df_line[['cum_dist', 'x_plot']].to_numpy()\n",
    "\n",
    "def euclidist(xy_array):\n",
    "    \"\"\"Takes array of x,y coord and calculates the euclidean distance between \n",
    "    successive pairs.\n",
    "    returns array of distances. first is zero, to maintain same length as coord array.\n",
    "    \"\"\"\n",
    "#     min_dist = 1.5 / len(xy_array)\n",
    "    dist = np.array([0])\n",
    "    prev_row = xy_array[0]\n",
    "    for row in xy_array[1:]:\n",
    "        x_diff = row[0] - prev_row[0]\n",
    "        y_diff = row[1] - prev_row[1]\n",
    "        dist = np.append(dist, np.sqrt(x_diff**2 + y_diff**2))\n",
    "        prev_row = row\n",
    "#     num_new_points = np.floor(np.array(dist / min_dist))\n",
    "    return dist\n",
    "\n",
    "def num_new_points(df, dist_array):\n",
    "    \"\"\"returns number of points to be added before each index of a\n",
    "    distance array.\n",
    "    Assumes total distance of 2 (between sqrt(2) and 2) for qq plot.\n",
    "    Adjust this number to change density of new points added.\n",
    "    \"\"\"\n",
    "    expected_total_dist = 2 #1.5\n",
    "    min_dist = expected_total_dist / get_max_samples(df, max_d3_rows)\n",
    "    return np.floor(np.array(dist_array / min_dist))\n",
    "\n",
    "# interp_input = np.column_stack((df_tmp, num_new_points(euclidist(df_tmp))))\n",
    "# interp_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_points(xy_array, num_new_points):\n",
    "    x0, y0 = xy_array[0]\n",
    "    new_points = None\n",
    "    iterator = enumerate(num_new_points)\n",
    "    for i, n in iterator:\n",
    "        if (i != 0):\n",
    "            if (n > 0):\n",
    "                x0, y0 = xy_array[i - 1]\n",
    "                x1, y1 = xy_array[i]\n",
    "                # divide new points along longest of x or axes\n",
    "                run = x0 - x1\n",
    "                rise = y0 - y1\n",
    "                if ((rise/run) < 1):\n",
    "                    _x = np.linspace(x0, x1, num=int(n + 2))\n",
    "                    _x = _x[1:-1]  # remove end points because they're not new points\n",
    "                    _y = np.interp(_x, [x0, x1], [y0, y1])\n",
    "                else:\n",
    "                    _y = np.linspace(y0, y1, num=int(n + 2))\n",
    "                    _y = _y[1:-1]  # remove end points because they're not new points\n",
    "                    _x = np.interp(_y, [y0, y1], [x0, x1])\n",
    "                interp_coords = np.column_stack((_x, _y))\n",
    "                if not isinstance(new_points, np.ndarray):\n",
    "                    new_points = interp_coords\n",
    "                else:\n",
    "                    new_points = np.vstack((new_points, interp_coords))\n",
    "#     print(\"interpolating \" + str(len(new_points)) + \" new points\")                    \n",
    "    return new_points \n",
    "\n",
    "# interp_points(df_tmp, num_new_points(euclidist(df_tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interp_points_df(df_downsampled, interp_units):\n",
    "    \"\"\"Takes dataframe of processed points for d3 of qq plots.\n",
    "    returns another df with same columns but interpolated \n",
    "    data points to fill in gaps in raw data. Interpolated points\n",
    "    aid in selecting in interactive d3 charts.\n",
    "    \"\"\"\n",
    "    tmp_downsampled_df = df_downsampled.iloc[0, :].copy()\n",
    "    col_names = df_downsampled.columns\n",
    "    # print(col_names)\n",
    "    # print(col_names.to_list().index('cum_dist'))\n",
    "\n",
    "    interp_points_for_df = None\n",
    "\n",
    "    for unit, tx in interp_units:\n",
    "#         print(\"Unit \" + str(unit) + \" \" + tx)\n",
    "        df_line = df_downsampled[(df_downsampled.unit==unit) & (df_downsampled.tx==tx)]\n",
    "        xy_array = df_line[['x_plot', 'cum_dist']].to_numpy()\n",
    "    #     line_euclidist = euclidist(df_line[['cum_dist', 'x_plot']].to_numpy())\n",
    "        new_points = interp_points(xy_array, num_new_points(df_downsampled, euclidist(xy_array)))\n",
    "        if isinstance(new_points, Iterable):\n",
    "        #     print(\"orig num points \" + str(len(xy_array)))\n",
    "        #     print(new_points)\n",
    "            base_row = df_line.iloc[0:1,:].copy()\n",
    "            base_row['timestamp'] = np.nan\n",
    "            collect_rows = base_row  # remove this first dimension holding row after loop\n",
    "        #     print(base_row.values)\n",
    "            for x, y in new_points:\n",
    "                x_index = col_names.to_list().index('x_plot')\n",
    "                y_index = col_names.to_list().index('cum_dist')\n",
    "                new_row = base_row.values\n",
    "                new_row[0][x_index] = x\n",
    "                new_row[0][y_index] = y \n",
    "                collect_rows = np.vstack((collect_rows, new_row))\n",
    "            collect_rows = collect_rows[1:]\n",
    "            if not isinstance(interp_points_for_df, np.ndarray):\n",
    "                interp_points_for_df = collect_rows\n",
    "            else:\n",
    "                interp_points_for_df = np.vstack((interp_points_for_df, collect_rows))\n",
    "        \n",
    "    print(\"total interpolated points \" + str(interp_points_for_df.shape))      \n",
    "    interp_df = pd.DataFrame(data = interp_points_for_df, columns = col_names)\n",
    "\n",
    "    return interp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do   \n",
    "- add new dataframe of interp points\n",
    "- resort dataframe with interp points\n",
    "- test in d3 observable\n",
    "- try exporting without padding NaN. it might just work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process final array to export for d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_header(df):\n",
    "    \"\"\"Removes column headers not needed for export.\n",
    "    Changes name of x-value header.\"\"\"\n",
    "    df_d3 = df.copy()\n",
    "    df_d3.drop([\"begin\", \"end\", \"group_idx\"], axis=1, inplace=True)\n",
    "    df_d3.rename(columns={\"x_plot\": \"x_idx\"}, inplace=True)\n",
    "    return df_d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD NaN columns. Inefficient. unneeded?\n",
    "# create row with full features for csv export\n",
    "# this takes a second\n",
    "# are all the NaN necessary? I think javascript removes them now...\n",
    "\n",
    "def pad_data(df):\n",
    "    \"\"\"This pads the data with empty columns all filled with NaN values.\n",
    "    This is the format currently used by the d3 chart.\"\"\"\n",
    "    df_d3 = df.copy()\n",
    "    tx_list = df_d3.tx.unique()\n",
    "\n",
    "    for tx in tx_list: \n",
    "        df_d3[tx] = df_d3[['tx', 'cum_dist']].apply(\n",
    "            lambda df, tx: df['cum_dist'] if (df['tx']==tx) else np.nan,\n",
    "            axis=1, tx=tx)\n",
    "    return df_d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_d3_data(df_d3, expt_id):\n",
    "    print(expt_id)\n",
    "    save_csv_filepath_dir = '/Users/walter/Data/margolis/observable/'\n",
    "#     save_csv_filepath = save_csv_filepath_dir + 'qq_for_d3_10K_interp_' + expt_id + '.csv'\n",
    "\n",
    "    save_csv_filepath = save_csv_filepath_dir + 'd3_cum_fr_10K_interp_' + expt_id + '.csv'\n",
    "\n",
    "    df_d3.to_csv(save_csv_filepath, index = False)\n",
    "\n",
    "    print('\\tSaved ' + save_csv_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Editing export here\n",
    "\n",
    "```\n",
    "expt_id to slice_id\n",
    "then tuples (slice_id, unit_filename)\n",
    "\n",
    "first confirm input for get_mean_firing_rates() as id or filepath for \"expt_id\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_from_filelist(expt_list, datafile_args, interp=True):\n",
    "    \"\"\"\n",
    "    expt_list is list of tuples with [(slice_id, unit_filepath), ...]\n",
    "    \"\"\"\n",
    "    for slice_id, unit_file in expt_list:\n",
    "        df = data_to_qq(unit_file, datafile_args)\n",
    "        df_downsampled = downsample_data(df, max_d3_rows)\n",
    "        if interp:\n",
    "            interp_units = activity_gaps_unit_tx(df, get_max_samples(df, max_d3_rows))\n",
    "            ### HERE interp\n",
    "#             interp_units = hypoactive_unit_tx(df_downsampled, get_max_samples(df, max_d3_rows))\n",
    "            interp_points_df = get_interp_points_df(df_downsampled, interp_units)\n",
    "            df_d3 = df_downsampled.append(interp_points_df)\n",
    "            # sort interpolated date into order of full dataframe\n",
    "            df_d3.sort_values(by=['unit', 'x_plot'], inplace=True)\n",
    "        df_d3 = clean_column_header(df_d3)\n",
    "        df_d3 = pad_data(df_d3)\n",
    "\n",
    "        mean_firing_by_tx = get_mean_firing_rates(unit_file, datafile_args) # get_mean_firing_by_tx(units_data, tx_times)\n",
    "        firing_rate_df = get_firing_rate_df(df_d3, mean_firing_by_tx)\n",
    "        df_d3 = firing_rate_df.append(df_d3)\n",
    "        export_d3_data(df_d3, slice_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Main() here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total interpolated points (2385, 8)\n",
      "20211005_17h33m55s\n",
      "\tSaved /Users/walter/Data/margolis/observable/d3_cum_fr_10K_interp_20211005_17h33m55s.csv\n"
     ]
    }
   ],
   "source": [
    "max_d3_rows = 10000\n",
    "\n",
    "DATA_DIR = '/Users/walter/Data/med64/experiment/VTA_NMDA/20211005_17h33m55s/' # walter local\n",
    "SLICE_PARAMS_FILE = 'slice_parameters.yaml'\n",
    "yaml_file = os.path.join(DATA_DIR, SLICE_PARAMS_FILE)\n",
    "\n",
    "expt_list, tx_files, unit_files = find_expt_files(yaml_file)\n",
    "\n",
    "export_from_filelist([(expt_list, unit_files)], tx_files, expt_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20211005_17h33m55s\n",
      "/Users/walter/Data/med64/experiment/VTA_NMDA/20211005_17h33m55s/20211005_17h33m55s_units_ts.mat\n"
     ]
    }
   ],
   "source": [
    "print(expt_list)\n",
    "print(unit_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- previous run with earlier files used this kind of export list  \n",
    "\n",
    "`expt_list[3:]`\n",
    "```\n",
    "output: ['825_16h06m19s', '827_12h06m26s', '827_13h30m23s', '827_15h57m53s']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_attachment_list(expt_list):\n",
    "#     save_csv_filepath_dir = '/Users/walter/Data/margolis/observable/'\n",
    "    attach_list = \"[\"\n",
    "    for expt_id in expt_list:\n",
    "        filename = \"FileAttachment(\\'d3_cum_fr_10K_interp_\" + expt_id + \".csv\\')\"\n",
    "        attach_list += filename + \",\\n\"\n",
    "    attach_list += \"]\"\n",
    "#     return attach_list\n",
    "    print(attach_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FileAttachment('d3_cum_fr_10K_interp_2.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_0.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_2.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_1.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_1.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_0.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_0.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_5.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp__.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_1.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_7.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_h.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_3.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_3.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_m.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_5.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_5.csv'),\n",
      "FileAttachment('d3_cum_fr_10K_interp_s.csv'),\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "file_attachment_list(expt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Use observable.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 45\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 4\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 4\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 290\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 8\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 4\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 145\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 48\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 30\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 102\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 49\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 33\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 463\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 6\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 6\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 99\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 29\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 25\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 295\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 48\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 34\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 167\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 26\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 15\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 428\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 19\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 13\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 633\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 13\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 9\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 152\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 2697\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 11\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 1101\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 3\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 4\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 1606\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 6\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 3277\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 17\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 17\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 996\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 3\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 197\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 5\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 3\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 1573\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 225\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 4\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 454\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 905\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 47\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 146\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 252\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 67\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 211\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 36\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 406\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 63\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 12\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 6074\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 439\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 1558\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 14\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 333\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 56\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 27\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 374\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 37\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 10\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 2464\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 3\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 954\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 5\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 3\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 347\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 56\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 27\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 271\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 22\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 7\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 707\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 533\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 733\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 36\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 18\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 431\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 62\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 16\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 864\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 27\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 12\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 263\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 7\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 6\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 715\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 12\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 6\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 3692\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 3\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 3\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 646\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 341\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 260\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 12\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 6\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 1946\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 7\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 34\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 650\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 18\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 15\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 156\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 42\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 10\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 118\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 41\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 13\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 1442\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 306\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 411\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 14\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 6\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 1057\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 81\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 3\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 133\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 10\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 9\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 408\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 50\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 35\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 4555\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 198\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 865\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 4\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 5\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 628\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 4\n",
      "groups_with_gaps() --> len(segments): 31\n",
      "groups_with_gaps() --> len(spikes): 669\n",
      "get_interp_points_df() --> Unit 0 Baseline\n",
      "get_interp_points_df() --> Unit 0 NMDA\n",
      "get_interp_points_df() --> Unit 0 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 1 Baseline\n",
      "get_interp_points_df() --> Unit 1 NMDA\n",
      "get_interp_points_df() --> Unit 1 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 2 Baseline\n",
      "get_interp_points_df() --> Unit 4 NMDA\n",
      "get_interp_points_df() --> Unit 4 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 5 Baseline\n",
      "get_interp_points_df() --> Unit 5 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 6 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 7 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 8 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 9 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 10 Baseline\n",
      "get_interp_points_df() --> Unit 12 NMDA\n",
      "get_interp_points_df() --> Unit 12 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 13 NMDA\n",
      "get_interp_points_df() --> Unit 14 NMDA\n",
      "get_interp_points_df() --> Unit 15 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 16 NMDA\n",
      "get_interp_points_df() --> Unit 16 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 18 Baseline\n",
      "get_interp_points_df() --> Unit 18 NMDA\n",
      "get_interp_points_df() --> Unit 19 Baseline\n",
      "get_interp_points_df() --> Unit 20 Baseline\n",
      "get_interp_points_df() --> Unit 21 Baseline\n",
      "get_interp_points_df() --> Unit 22 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 26 Baseline\n",
      "get_interp_points_df() --> Unit 27 NMDA\n",
      "get_interp_points_df() --> Unit 28 NMDA\n",
      "get_interp_points_df() --> Unit 28 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 30 NMDA\n",
      "get_interp_points_df() --> Unit 30 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 32 NMDA\n",
      "get_interp_points_df() --> Unit 32 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 33 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 34 NMDA\n",
      "get_interp_points_df() --> Unit 34 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 35 NMDA\n",
      "get_interp_points_df() --> Unit 35 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 36 NMDA\n",
      "get_interp_points_df() --> Unit 36 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 37 NMDA\n",
      "get_interp_points_df() --> Unit 37 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 38 Baseline\n",
      "get_interp_points_df() --> Unit 39 NMDA\n",
      "get_interp_points_df() --> Unit 39 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 41 Baseline\n",
      "get_interp_points_df() --> Unit 42 NMDA\n",
      "get_interp_points_df() --> Unit 42 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 43 Baseline\n",
      "get_interp_points_df() --> Unit 46 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 47 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 48 Baseline\n",
      "get_interp_points_df() --> Unit 48 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 51 NMDA\n",
      "get_interp_points_df() --> Unit 51 NMDA Apamin\n",
      "get_interp_points_df() --> Unit 52 NMDA Apamin\n",
      "get_interp_points_df() --> is None: [[nan 0 'Baseline' ... nan 0.0033222591362126247 0.056686256427453605]\n",
      " [nan 0 'Baseline' ... nan 0.006644518272425249 0.11337251285490721]\n",
      " [nan 0 'Baseline' ... nan 0.009966777408637873 0.1700587692823608]\n",
      " ...\n",
      " [nan 52 'NMDA Apamin' ... nan 1.0 0.8126621405492731]\n",
      " [nan 52 'NMDA Apamin' ... nan 1.0 0.8751080936995155]\n",
      " [nan 52 'NMDA Apamin' ... nan 1.0 0.9375540468497578]]\n",
      "total interpolated points (1115, 8)\n",
      "\tSaved /Users/walter/Data/margolis/observable/d3_cum_fr_10K_interp_debug_20211005_17h33m55s.csv\n"
     ]
    }
   ],
   "source": [
    "from observable import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Debugging Code Below\n",
    "- Use these cells to see inside functions defined above for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find local data filenames ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define global file path\n",
    "\n",
    "# DATA_DIR = '/Users/walter/Data/med64/experiment/HB_139_DAMGO/825_12h24m37s'\n",
    "DATA_DIR = '/Users/walter/Data/med64/experiment/VTA_NMDA/20211005_17h33m55s/' # walter local\n",
    "\n",
    "SLICE_PARAMS_FILE = 'slice_parameters.yaml'\n",
    "PRODUCT_DIR = '/Users/walter/Data/med64/product/HB_139_DAMGO/'\n",
    "\n",
    "\n",
    "yaml_file = os.path.join(DATA_DIR, SLICE_PARAMS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Which param file to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0pc0JymLxQ5",
    "outputId": "cee167c2-027e-4e70-b871-0c6646e57383"
   },
   "outputs": [],
   "source": [
    "# expt_list, tx_files, unit_files = find_better_expt_files(DATA_DIR)\n",
    "splice_params_filepath = os.path.join(DATA_DIR, SLICE_PARAMS_FILE)\n",
    "expt_list, tx_files, unit_files = find_expt_files(splice_params_filepath)\n",
    "\n",
    "# datafile_args = dict(data_dir = DATA_DIR, unit_files = unit_files, tx_files = tx_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/walter/Data/med64/experiment/HB_139_DAMGO/825_12h24m37s/slice_parameters.yaml\n",
      "/Users/walter/Data/med64/experiment/VTA_NMDA/20211005_17h33m55s/slice_parameters.yaml\n"
     ]
    }
   ],
   "source": [
    "print(yaml_file)\n",
    "print(splice_params_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0pc0JymLxQ5",
    "outputId": "cee167c2-027e-4e70-b871-0c6646e57383"
   },
   "source": [
    "```python\n",
    "# Original code for older files\n",
    "expt_list, tx_files, unit_files = find_expt_files(DATA_DIR)\n",
    "datafile_args = dict(data_dir = DATA_DIR, unit_files = unit_files, tx_files = tx_files)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QQ Plot ##\n",
    "- get timestamps for tx\n",
    "- rank timestamps as lin_space from 0 to  1\n",
    "- rank time in tx as lin_space from start to finish\n",
    "- plot (ranks of tx, ranks of timestamps )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/walter/Data/med64/experiment/VTA_NMDA/20211005_17h33m55s/20211005_17h33m55s_treatments.csv\n",
      "/Users/walter/Data/med64/experiment/VTA_NMDA/20211005_17h33m55s/20211005_17h33m55s_units_ts.mat\n"
     ]
    }
   ],
   "source": [
    "print(tx_files)\n",
    "print(unit_files)\n",
    "\n",
    "units_data, tx_times = get_expt_data(unit_files, tx_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firing Rate Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "expt_mean_fr, unit_mean_fr = mean_firing_rates(units_data, tx_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get_TX_Ranges   \n",
    "- get_tx_rangesget_tx_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Baseline': [6.0, 645.0],\n",
       " 'NMDA': [645.0, 1281.0],\n",
       " 'NMDA Apamin': [1281.0, 1900.0],\n",
       " 'TTX': [1900.0, 2550.0]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tx_range_tmp = get_tx_ranges(tx_times)\n",
    "# tx_range_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "### KEEP ###\n",
    "# Keep this for constant used in num_new_points()\n",
    "\n",
    "max_d3_rows = 10000\n",
    "MAX_SAMPLES = 32 # default when no df for get_max_samples(df, max_d3_rows)\n",
    "print(MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run by steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = data_to_qq(expt_id, datafile_args)\n",
    "df = data_to_qq(unit_files, tx_files)\n",
    "\n",
    "df_downsampled = downsample_data(df, max_d3_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Baseline'),\n",
       " (0, 'NMDA'),\n",
       " (0, 'NMDA Apamin'),\n",
       " (1, 'Baseline'),\n",
       " (1, 'NMDA'),\n",
       " (1, 'NMDA Apamin'),\n",
       " (2, 'Baseline'),\n",
       " (4, 'NMDA'),\n",
       " (4, 'NMDA Apamin'),\n",
       " (5, 'Baseline'),\n",
       " (5, 'NMDA Apamin'),\n",
       " (6, 'NMDA Apamin'),\n",
       " (7, 'NMDA Apamin'),\n",
       " (8, 'NMDA Apamin'),\n",
       " (9, 'NMDA Apamin'),\n",
       " (10, 'Baseline'),\n",
       " (12, 'NMDA'),\n",
       " (12, 'NMDA Apamin'),\n",
       " (13, 'NMDA'),\n",
       " (14, 'NMDA'),\n",
       " (15, 'NMDA Apamin'),\n",
       " (16, 'NMDA'),\n",
       " (16, 'NMDA Apamin'),\n",
       " (18, 'Baseline'),\n",
       " (18, 'NMDA'),\n",
       " (19, 'Baseline'),\n",
       " (20, 'Baseline'),\n",
       " (21, 'Baseline'),\n",
       " (22, 'NMDA Apamin'),\n",
       " (26, 'Baseline'),\n",
       " (27, 'NMDA'),\n",
       " (28, 'NMDA'),\n",
       " (28, 'NMDA Apamin'),\n",
       " (30, 'NMDA'),\n",
       " (30, 'NMDA Apamin'),\n",
       " (32, 'NMDA'),\n",
       " (32, 'NMDA Apamin'),\n",
       " (33, 'NMDA Apamin'),\n",
       " (34, 'NMDA'),\n",
       " (34, 'NMDA Apamin'),\n",
       " (35, 'NMDA'),\n",
       " (35, 'NMDA Apamin'),\n",
       " (36, 'NMDA'),\n",
       " (36, 'NMDA Apamin'),\n",
       " (37, 'NMDA'),\n",
       " (37, 'NMDA Apamin'),\n",
       " (38, 'Baseline'),\n",
       " (39, 'NMDA'),\n",
       " (39, 'NMDA Apamin'),\n",
       " (41, 'Baseline'),\n",
       " (42, 'NMDA'),\n",
       " (42, 'NMDA Apamin'),\n",
       " (43, 'Baseline'),\n",
       " (46, 'NMDA Apamin'),\n",
       " (47, 'NMDA Apamin'),\n",
       " (48, 'Baseline'),\n",
       " (48, 'NMDA Apamin'),\n",
       " (51, 'NMDA'),\n",
       " (51, 'NMDA Apamin'),\n",
       " (52, 'NMDA Apamin')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interp_units = activity_gaps_unit_tx(df, get_max_samples(df, max_d3_rows))\n",
    "# interp_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tx_filter_str = 'Baseline'  # 'AMPA blocker DNQX'\n",
    "    # Baseline', 'NMDA Apamin', 'NMDA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>unit</th>\n",
       "      <th>tx</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>group_idx</th>\n",
       "      <th>cum_dist</th>\n",
       "      <th>x_plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54071</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>6.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>81.046925</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>6.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.117444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>131.508075</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>6.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.027972</td>\n",
       "      <td>0.196413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>182.912075</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>6.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.041958</td>\n",
       "      <td>0.276858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>207.410925</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>6.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.055944</td>\n",
       "      <td>0.315197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>538.672575</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>6.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.833603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>542.278375</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>6.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.839246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>543.579525</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>6.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.841283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>550.752200</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>6.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.852507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53931</th>\n",
       "      <td>645.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>6.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp  unit        tx  begin    end  group_idx  cum_dist    x_plot\n",
       "54071    6.000000     2  Baseline    6.0  645.0        NaN  0.000000  0.000000\n",
       "344     81.046925     2  Baseline    6.0  645.0        2.0  0.013986  0.117444\n",
       "346    131.508075     2  Baseline    6.0  645.0        4.0  0.027972  0.196413\n",
       "348    182.912075     2  Baseline    6.0  645.0        6.0  0.041958  0.276858\n",
       "350    207.410925     2  Baseline    6.0  645.0        8.0  0.055944  0.315197\n",
       "...           ...   ...       ...    ...    ...        ...       ...       ...\n",
       "478    538.672575     2  Baseline    6.0  645.0      136.0  0.951049  0.833603\n",
       "480    542.278375     2  Baseline    6.0  645.0      138.0  0.965035  0.839246\n",
       "482    543.579525     2  Baseline    6.0  645.0      140.0  0.979021  0.841283\n",
       "484    550.752200     2  Baseline    6.0  645.0      142.0  0.993007  0.852507\n",
       "53931  645.000000     2  Baseline    6.0  645.0        NaN  1.000000  1.000000\n",
       "\n",
       "[73 rows x 8 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# troubleshooting to verify integrity of data at intermediate steps\n",
    "df[(df.unit==2) & (df.tx==tx_filter_str)]\n",
    "df_downsampled[(df_downsampled.unit==2) & (df_downsampled.tx==tx_filter_str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total interpolated points (2385, 8)\n"
     ]
    }
   ],
   "source": [
    "# interp_units = hypoactive_unit_tx(df_downsampled, get_max_samples(df, max_d3_rows))\n",
    "max_d3_samples = get_max_samples(df, max_d3_rows)\n",
    "interp_units = activity_gaps_unit_tx(df, max_d3_samples)\n",
    "interp_points_df = get_interp_points_df(df_downsampled, interp_units)\n",
    "\n",
    "df_d3 = df_downsampled.append(interp_points_df)\n",
    "\n",
    "# sort interpolated date into order of full dataframe\n",
    "df_d3.sort_values(by=['unit', 'x_plot'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d3 = clean_column_header(df_d3)\n",
    "df_d3 = pad_data(df_d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Baseline', 'NMDA', 'NMDA Apamin'], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tx.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>unit</th>\n",
       "      <th>tx</th>\n",
       "      <th>cum_dist</th>\n",
       "      <th>x_idx</th>\n",
       "      <th>Baseline</th>\n",
       "      <th>NMDA</th>\n",
       "      <th>NMDA Apamin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54071</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>0.029361</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.058722</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.01049</td>\n",
       "      <td>0.088083</td>\n",
       "      <td>0.010490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>81.046925</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.117444</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.994406</td>\n",
       "      <td>0.882006</td>\n",
       "      <td>0.994406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.995804</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.995804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.997203</td>\n",
       "      <td>0.941003</td>\n",
       "      <td>0.997203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.970501</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53931</th>\n",
       "      <td>645.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp unit        tx  cum_dist     x_idx  Baseline  NMDA  \\\n",
       "54071        6.0    2  Baseline       0.0       0.0  0.000000   NaN   \n",
       "255          NaN    2  Baseline  0.003497  0.029361  0.003497   NaN   \n",
       "256          NaN    2  Baseline  0.006993  0.058722  0.006993   NaN   \n",
       "257          NaN    2  Baseline   0.01049  0.088083  0.010490   NaN   \n",
       "344    81.046925    2  Baseline  0.013986  0.117444  0.013986   NaN   \n",
       "...          ...  ...       ...       ...       ...       ...   ...   \n",
       "268          NaN    2  Baseline  0.994406  0.882006  0.994406   NaN   \n",
       "269          NaN    2  Baseline  0.995804  0.911504  0.995804   NaN   \n",
       "270          NaN    2  Baseline  0.997203  0.941003  0.997203   NaN   \n",
       "271          NaN    2  Baseline  0.998601  0.970501  0.998601   NaN   \n",
       "53931      645.0    2  Baseline       1.0       1.0  1.000000   NaN   \n",
       "\n",
       "       NMDA Apamin  \n",
       "54071          NaN  \n",
       "255            NaN  \n",
       "256            NaN  \n",
       "257            NaN  \n",
       "344            NaN  \n",
       "...            ...  \n",
       "268            NaN  \n",
       "269            NaN  \n",
       "270            NaN  \n",
       "271            NaN  \n",
       "53931          NaN  \n",
       "\n",
       "[90 rows x 8 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_d3[(df_d3.unit==2) & (df_d3.tx==tx_filter_str)] # DAMGO 500nM On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>unit</th>\n",
       "      <th>tx</th>\n",
       "      <th>cum_dist</th>\n",
       "      <th>x_idx</th>\n",
       "      <th>Baseline</th>\n",
       "      <th>NMDA</th>\n",
       "      <th>NMDA Apamin</th>\n",
       "      <th>Firing Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.067293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.450704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.223787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.156495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.721440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  timestamp unit        tx cum_dist x_idx  Baseline  NMDA  NMDA Apamin  \\\n",
       "0       NaN    0  Baseline      NaN   NaN       NaN   NaN          NaN   \n",
       "1       NaN    1  Baseline      NaN   NaN       NaN   NaN          NaN   \n",
       "2       NaN    2  Baseline      NaN   NaN       NaN   NaN          NaN   \n",
       "3       NaN    3  Baseline      NaN   NaN       NaN   NaN          NaN   \n",
       "4       NaN    4  Baseline      NaN   NaN       NaN   NaN          NaN   \n",
       "\n",
       "   Firing Rate  \n",
       "0     0.067293  \n",
       "1     0.450704  \n",
       "2     0.223787  \n",
       "3     0.156495  \n",
       "4     0.721440  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_firing_by_tx = get_mean_firing_by_tx(units_data, tx_times)\n",
    "firing_rate_df = get_firing_rate_df(df_d3, mean_firing_by_tx)\n",
    "firing_rate_df.append(df_d3).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- change export_from_filelist(expt_list...) to take single string unit_file instead of list\n",
    "- change unit_files and tx_files to singular since they are string value now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Margolis_file_load_explore.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
